{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week11_Discussion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAb0w39FmSTdFirGo4uShT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrinmayeeKulkarni/Cloud-Computing-Weekly-Discussion/blob/master/Week11_Discussion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuFYfeJeeOqP",
        "colab_type": "text"
      },
      "source": [
        "### What problems does Hadoop Solve?\n",
        "Flexibility due to variety- Hadoop accepts data from variety of sources and formats (structured and unstructured). This means organizations can use Hadoop to derive valuable insights from data sources such as social media, email conversations.  Hadoop can be used for a wide variety of purposes, such as log processing, recommendation systems, data warehousing, market campaign analysis and fraud detection.\n",
        "\n",
        "* Clustering system- It is cost effective as it uses cluster of commodity hardware and so can be cheap to store data.\n",
        "\n",
        "* Performance- It is distributed processing and storage architecture and processes vast amounts of data very fast. It divides data into batches and distributes it to the nodes in the cluster with the task code through MapReduce to execute codes in parallel. \n",
        "\n",
        "* Resilient to failures- The key advantage is fault tolerance due to redundancy. As data is sent to a node, it is replicated and therefore in case of a failure can be retrieved.\n",
        "\n",
        "* Open source- The code is easily available due to open source technology and can be used to customize requirements.\n",
        "\n",
        "* Scalable- It is highly scalable as; it can store and distribute very large data sets across hundreds of inexpensive servers that operate in parallel.\n",
        "\n",
        "* Multiple language supported platform \n",
        "\n",
        "### What are the key differences between Hadoop and Spark?\n",
        "Hadoop is a disk-based batch processing system which reads data from disk and saves the results back to the disk. But many times, extremely fast streaming applications require real-time data would not work well with disk-based system.\n",
        "\n",
        "* Spark is easier to program and doesn’t require abstraction.\n",
        "\n",
        "* Due to replication as fault tolerance Hadoop has more disk-based overhead. Resilient distributed databases RDDs like Spark eliminate this overhead by being in-memory (using disk only when data would not fit in memory). Spark has this interesting way of fault tolerance by remembering the steps used to create the RDDs so it can be rebuilt in case of failure.\n",
        "\n",
        "* Programmers can perform streaming, batch processing and machine learning ,all in the same cluster.\n",
        "\n",
        "* Executes jobs 10-100 time faster than Hadoop MapReduce.\n",
        "\n",
        "### Post a screenshot of a lab where you had difficulty with a concept or learned something.\n",
        "\n",
        "The Analyze Big Data with Hadoop lab access was denied from my account , so I couldn’t attempt it. I went through the other three labs, \n",
        "  * Introduction to Cloud Dataproc: Hadoop and Spark on Google Cloud Platform;\n",
        "\n",
        "  * Dataproc: Qwik Start - Command Line\n",
        "  * Provisioning and Using a Managed Hadoop/Spark Cluster with Cloud Dataproc (Command Line);\n",
        "   \n",
        "   I also attempted,\n",
        "  * Launching Dataproc Jobs with Cloud Composer,\n",
        "  \n",
        "   but had issues in constructing the URL to trigger the DAG. I used all the permutations and combinations to add the values for AIRFLOW_URI and CLIENT_ID but I was getting syntax issues many times and when not that I was getting this particular error all the time,\n",
        "\n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031623-0c3a7c80-7b6e-11ea-8486-c38c035a8f02.png)\n",
        "\n",
        "   Creating a Composer Environment-\n",
        "\n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031628-13618a80-7b6e-11ea-9c29-b3882672e5c5.png)\n",
        "\n",
        "   Setting Airflow variables-\n",
        "\n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031632-1a889880-7b6e-11ea-91f6-b73521f8471b.png)\n",
        "\n",
        "   Viewing the file running-\n",
        "\n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031645-24aa9700-7b6e-11ea-84fd-1af4857eb80e.png)\n",
        "\n",
        "   Running Graph View-\n",
        "\n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031651-2f652c00-7b6e-11ea-9f34-0472d7803572.png)\n",
        "\n",
        "   Provisioning and Using a Managed Hadoop/Spark Cluster with Cloud Dataproc (Command Line)-\n",
        "\n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031657-3855fd80-7b6e-11ea-8b45-2a732bd6542d.png)\n",
        "\n",
        "   Pre-emptible node\n",
        "   \n",
        "   ![alt text](https://user-images.githubusercontent.com/44381361/79031661-3f7d0b80-7b6e-11ea-8106-9f3badb2c3fe.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}